{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29f31a81",
   "metadata": {},
   "source": [
    "# Dataset link: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "# Glove file: https://nlp.stanford.edu/projects/glove/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6401bc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\archa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\archa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#importing all required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "from numpy.random import RandomState\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import string\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize \n",
    "nltk.download('punkt')\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, recall_score,precision_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "179dd02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to prepare the dataset required\n",
    "\n",
    "class Dataset:\n",
    "    def __init__(self, df = None):\n",
    "        self.df = df\n",
    "            \n",
    "    def read_data(self, file):    \n",
    "        self.df = pd.read_csv(file, sep ='\\t',names=['label', 'text'])\n",
    "        print(\"\\nGiven UCI Dataset:\\n\\n\", self.df.head)\n",
    "        self.df = self.df.replace(['ham','spam'],[0, 1])\n",
    "        self.plot_class()\n",
    "        \n",
    "    #plotting class distribution\n",
    "    def plot_class(self): \n",
    "        print(\"\\nClass distribution:\\n\", self.df['label'].value_counts())\n",
    "        self.df['label'].value_counts().plot(kind = 'bar',color = [\"green\",\"red\"])\n",
    "        plt.title('Class distribution')\n",
    "        plt.show()\n",
    "       \n",
    "    #performing preprocessing on text data\n",
    "    def preprocess(self):\n",
    "        #removing punctuations\n",
    "        self.df[\"text1\"] = self.df[\"text\"].apply(lambda text: text.translate(str.maketrans('', '', string.punctuation)))\n",
    "        \n",
    "        #removing stop words\n",
    "        STOPWORDS = set(stopwords.words('english'))\n",
    "        self.df[\"text2\"] = self.df[\"text1\"].apply(lambda text1: \" \".join([word for word in str(text1).split() if word not in STOPWORDS]))\n",
    "        \n",
    "        #performing stemming\n",
    "        self.df[\"text3\"] = self.df[\"text2\"].apply(lambda text2: \" \".join([PorterStemmer().stem(word) for word in text2.split()]))\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    #splitting into train and test\n",
    "    def split(self):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(self.df['text3'], self.df['label'], test_size = 0.3, random_state = 37)\n",
    "        print (\"X_train size: \", len(X_train))\n",
    "        print(\"X_test size: \", len(X_test))\n",
    "        print(\"y_train size: \", len(y_train))\n",
    "        print(\"y_test size: \", len(y_test))\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619d21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to define Traditional Machine Learning Classifiers from SKLearn\n",
    "\n",
    "class Spam_Classification_ML:\n",
    "    \n",
    "    def __init__(self, X_train, X_test, y_train, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    def prepare_models(self):\n",
    "        models = {} \n",
    "        #LR\n",
    "        models['Logistic_Regression'] = LogisticRegression(max_iter = 80, solver = 'newton-cg')\n",
    "        #SVC\n",
    "        models['SVC'] = SVC(gamma = 'auto')\n",
    "        #MNB\n",
    "        models['MultinomialNB'] = MultinomialNB(alpha = 0.5)\n",
    "        #DT\n",
    "        models['Decision_Tree'] = DecisionTreeClassifier(criterion = 'gini')\n",
    "        #KNN\n",
    "        models['KNN'] = KNeighborsClassifier(weights = 'distance')     \n",
    "        #RF\n",
    "        models['RandomForest'] = RandomForestClassifier(max_features = 'log2', criterion = 'gini')\n",
    "        return models\n",
    "    \n",
    "    def evaluate(self, models):\n",
    "        for model in models:\n",
    "            y_pred = None\n",
    "            print(\"\\n\\nRunning model: {}\".format(model))\n",
    "            current_model = models[model]\n",
    "            \n",
    "            #vectorizing the text data\n",
    "            cv = CountVectorizer(max_features = 1500)\n",
    "            cv.fit(self.X_train)\n",
    "            X_train_cv = cv.transform(self.X_train)\n",
    "            X_test_cv = cv.transform(self.X_test)\n",
    "            \n",
    "            current_model.fit(X_train_cv, self.y_train)\n",
    "            y_pred = current_model.predict(X_test_cv)\n",
    "            print(\" Train accuracy is {:.2f}\".format(current_model.score(X_train_cv, self.y_train) * 100))\n",
    "            print(\" Test accuracy is {:.2f}\".format(accuracy_score(self.y_test, y_pred) * 100))\n",
    "            print(\" F1 Score: {:.2f}\".format(f1_score(self.y_test, y_pred, average='macro') * 100))\n",
    "            print(\" Precision Score: {:.2f}\".format(precision_score(self.y_test, y_pred, average='macro') * 100))\n",
    "            print(\" Recall Score: {:.2f}\".format(recall_score(self.y_test, y_pred, average='macro') * 100))\n",
    "            print(classification_report(self.y_test, y_pred))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0005a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class to define Word Embedding Layers using GLOVE with Deep Learning\n",
    "\n",
    "class Spam_Classification_DL:\n",
    "    \n",
    "    def __init__(self, df, X_train, X_test, y_train, y_test):\n",
    "        self.df = df\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test = y_test\n",
    "    \n",
    "    #creating embedding vector from the given glove.6B.100d.txt\n",
    "    def glove(self):\n",
    "        embedding_vector = {}\n",
    "        f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embedding_vector[word] = coefs\n",
    "        f.close()\n",
    "        print('\\nFound %s word vectors in glove.\\n' % len(embedding_vector))\n",
    "        self.matrix(embedding_vector)\n",
    "        \n",
    "    #tokenizing the text data\n",
    "    def token(self):\n",
    "        tok = Tokenizer()\n",
    "        tok.fit_on_texts(self.df[\"text3\"])\n",
    "        return tok\n",
    "    \n",
    "    #creating embedding matrix\n",
    "    def matrix(self, embedding_vector):\n",
    "        tok = self.token()\n",
    "        self.vocab_size = len(tok.word_index) + 1\n",
    "\n",
    "        self.embedding_matrix = np.zeros((self.vocab_size, 100))\n",
    "        for word, i in tok.word_index.items():\n",
    "            embedding_value = embedding_vector.get(word)\n",
    "            if embedding_value is not None:\n",
    "                self.embedding_matrix[i] = embedding_value\n",
    "\n",
    "        print(\"Embedding matrix shape\\n\",self.embedding_matrix.shape)      \n",
    "    \n",
    "    \n",
    "    #sequencing : each sentence represented by sequences of numbers using texts_to_sequences from tokenizer object\n",
    "    #padding : we padded the sequence so that we can have same length of each sequence\n",
    "    #Sequencing and padding on training and testing \n",
    "    def prepare_train_test(self):\n",
    "        tokenizer = Tokenizer(num_words = self.vocab_size, char_level=False, oov_token = \"<OOV>\")\n",
    "        tokenizer.fit_on_texts(self.X_train)\n",
    "        word_index = tokenizer.word_index\n",
    "        tot_words = len(word_index)\n",
    "        print('There are %s unique tokens in training data. \\n' % tot_words)\n",
    "        \n",
    "        train_seq = tokenizer.texts_to_sequences(self.X_train)\n",
    "        train_padded = pad_sequences(train_seq, maxlen = 50, \n",
    "                                         padding = \"post\", truncating = \"post\")\n",
    "\n",
    "        test_seq = tokenizer.texts_to_sequences(self.X_test)\n",
    "        test_padded = pad_sequences(test_seq, maxlen = 50,\n",
    "                                      padding = \"post\", truncating = \"post\")\n",
    "        return train_padded, test_padded\n",
    "    \n",
    "    #Dense model architecture with two dense layers\n",
    "    def model(self):\n",
    "        train_padded, test_padded = self.prepare_train_test()\n",
    "        model = Sequential()\n",
    "        #glove embedding layer\n",
    "        model.add(Embedding(self.vocab_size, 100, weights = [self.embedding_matrix], input_length=50, trainable = False))\n",
    "        \n",
    "        model.add(GlobalAveragePooling1D())\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        model.compile(loss='binary_crossentropy',optimizer='adam' ,metrics=['accuracy'])\n",
    "        early_stop = EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(train_padded, self.y_train, epochs=30, \n",
    "                  validation_data=(test_padded, self.y_test),callbacks =[early_stop], verbose=2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61fba367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PART1\n",
      "\n",
      "SPAM classification using Traditional Machine Learning Classifiers from SKLearn\n",
      "\n",
      "\n",
      "\n",
      "Given UCI Dataset:\n",
      "\n",
      " <bound method NDFrame.head of      label                                               text\n",
      "0      ham  Go until jurong point, crazy.. Available only ...\n",
      "1      ham                      Ok lar... Joking wif u oni...\n",
      "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3      ham  U dun say so early hor... U c already then say...\n",
      "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
      "...    ...                                                ...\n",
      "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
      "5568   ham               Will Ã¼ b going to esplanade fr home?\n",
      "5569   ham  Pity, * was in mood for that. So...any other s...\n",
      "5570   ham  The guy did some bitching but I acted like i'd...\n",
      "5571   ham                         Rofl. Its true to its name\n",
      "\n",
      "[5572 rows x 2 columns]>\n",
      "\n",
      "Class distribution:\n",
      " 0    4825\n",
      "1     747\n",
      "Name: label, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train size:  3900\n",
      "X_test size:  1672\n",
      "y_train size:  3900\n",
      "y_test size:  1672\n",
      "\n",
      "\n",
      "Running model: Logistic_Regression\n",
      " Train accuracy is 99.23\n",
      " Test accuracy is 98.09\n",
      " F1 Score: 95.95\n",
      " Precision Score: 97.92\n",
      " Recall Score: 94.20\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99      1432\n",
      "           1       0.98      0.89      0.93       240\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.98      0.94      0.96      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n",
      "\n",
      "\n",
      "Running model: SVC\n",
      " Train accuracy is 87.00\n",
      " Test accuracy is 85.65\n",
      " F1 Score: 46.13\n",
      " Precision Score: 42.82\n",
      " Recall Score: 50.00\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      1.00      0.92      1432\n",
      "           1       0.00      0.00      0.00       240\n",
      "\n",
      "    accuracy                           0.86      1672\n",
      "   macro avg       0.43      0.50      0.46      1672\n",
      "weighted avg       0.73      0.86      0.79      1672\n",
      "\n",
      "\n",
      "\n",
      "Running model: MultinomialNB\n",
      " Train accuracy is 98.54\n",
      " Test accuracy is 98.39\n",
      " F1 Score: 96.70\n",
      " Precision Score: 96.95\n",
      " Recall Score: 96.46\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99      1432\n",
      "           1       0.95      0.94      0.94       240\n",
      "\n",
      "    accuracy                           0.98      1672\n",
      "   macro avg       0.97      0.96      0.97      1672\n",
      "weighted avg       0.98      0.98      0.98      1672\n",
      "\n",
      "\n",
      "\n",
      "Running model: Decision_Tree\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\archa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\archa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\archa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\archa\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train accuracy is 99.95\n",
      " Test accuracy is 95.45\n",
      " F1 Score: 90.79\n",
      " Precision Score: 90.65\n",
      " Recall Score: 90.93\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97      1432\n",
      "           1       0.84      0.85      0.84       240\n",
      "\n",
      "    accuracy                           0.95      1672\n",
      "   macro avg       0.91      0.91      0.91      1672\n",
      "weighted avg       0.95      0.95      0.95      1672\n",
      "\n",
      "\n",
      "\n",
      "Running model: KNN\n",
      " Train accuracy is 99.95\n",
      " Test accuracy is 93.96\n",
      " F1 Score: 85.04\n",
      " Precision Score: 96.38\n",
      " Recall Score: 79.13\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.97      1432\n",
      "           1       0.99      0.58      0.73       240\n",
      "\n",
      "    accuracy                           0.94      1672\n",
      "   macro avg       0.96      0.79      0.85      1672\n",
      "weighted avg       0.94      0.94      0.93      1672\n",
      "\n",
      "\n",
      "\n",
      "Running model: RandomForest\n",
      " Train accuracy is 99.95\n",
      " Test accuracy is 97.49\n",
      " F1 Score: 94.48\n",
      " Precision Score: 98.58\n",
      " Recall Score: 91.25\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99      1432\n",
      "           1       1.00      0.82      0.90       240\n",
      "\n",
      "    accuracy                           0.97      1672\n",
      "   macro avg       0.99      0.91      0.94      1672\n",
      "weighted avg       0.98      0.97      0.97      1672\n",
      "\n",
      "\n",
      "\n",
      "PART2\n",
      "\n",
      "SPAM classification using Use GLOVE Word Embedding Layers with Deep Learning\n",
      "\n",
      "\n",
      "\n",
      "Found 400000 word vectors in glove.\n",
      "\n",
      "Embedding matrix shape\n",
      " (8278, 100)\n",
      "There are 6799 unique tokens in training data. \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           827800    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 100)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 24)                2424      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 25        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 830,249\n",
      "Trainable params: 2,449\n",
      "Non-trainable params: 827,800\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/30\n",
      "122/122 - 1s - loss: 0.5350 - accuracy: 0.8656 - val_loss: 0.5183 - val_accuracy: 0.8565 - 577ms/epoch - 5ms/step\n",
      "Epoch 2/30\n",
      "122/122 - 0s - loss: 0.4256 - accuracy: 0.8700 - val_loss: 0.4127 - val_accuracy: 0.8565 - 136ms/epoch - 1ms/step\n",
      "Epoch 3/30\n",
      "122/122 - 0s - loss: 0.3384 - accuracy: 0.8746 - val_loss: 0.3200 - val_accuracy: 0.8690 - 136ms/epoch - 1ms/step\n",
      "Epoch 4/30\n",
      "122/122 - 0s - loss: 0.2873 - accuracy: 0.8885 - val_loss: 0.2792 - val_accuracy: 0.8846 - 136ms/epoch - 1ms/step\n",
      "Epoch 5/30\n",
      "122/122 - 0s - loss: 0.2625 - accuracy: 0.8964 - val_loss: 0.2609 - val_accuracy: 0.8923 - 128ms/epoch - 1ms/step\n",
      "Epoch 6/30\n",
      "122/122 - 0s - loss: 0.2452 - accuracy: 0.9026 - val_loss: 0.2473 - val_accuracy: 0.8965 - 136ms/epoch - 1ms/step\n",
      "Epoch 7/30\n",
      "122/122 - 0s - loss: 0.2367 - accuracy: 0.9085 - val_loss: 0.2337 - val_accuracy: 0.9061 - 128ms/epoch - 1ms/step\n",
      "Epoch 8/30\n",
      "122/122 - 0s - loss: 0.2285 - accuracy: 0.9172 - val_loss: 0.2286 - val_accuracy: 0.9049 - 128ms/epoch - 1ms/step\n",
      "Epoch 9/30\n",
      "122/122 - 0s - loss: 0.2148 - accuracy: 0.9200 - val_loss: 0.2217 - val_accuracy: 0.9127 - 128ms/epoch - 1ms/step\n",
      "Epoch 10/30\n",
      "122/122 - 0s - loss: 0.2147 - accuracy: 0.9203 - val_loss: 0.2130 - val_accuracy: 0.9181 - 136ms/epoch - 1ms/step\n",
      "Epoch 11/30\n",
      "122/122 - 0s - loss: 0.2086 - accuracy: 0.9269 - val_loss: 0.2104 - val_accuracy: 0.9163 - 136ms/epoch - 1ms/step\n",
      "Epoch 12/30\n",
      "122/122 - 0s - loss: 0.2006 - accuracy: 0.9274 - val_loss: 0.2154 - val_accuracy: 0.9157 - 152ms/epoch - 1ms/step\n",
      "Epoch 13/30\n",
      "122/122 - 0s - loss: 0.2001 - accuracy: 0.9292 - val_loss: 0.2012 - val_accuracy: 0.9193 - 168ms/epoch - 1ms/step\n",
      "Epoch 14/30\n",
      "122/122 - 0s - loss: 0.1947 - accuracy: 0.9313 - val_loss: 0.2087 - val_accuracy: 0.9217 - 170ms/epoch - 1ms/step\n",
      "Epoch 15/30\n",
      "122/122 - 0s - loss: 0.1926 - accuracy: 0.9321 - val_loss: 0.2058 - val_accuracy: 0.9217 - 172ms/epoch - 1ms/step\n",
      "Epoch 16/30\n",
      "122/122 - 0s - loss: 0.1871 - accuracy: 0.9346 - val_loss: 0.1961 - val_accuracy: 0.9228 - 158ms/epoch - 1ms/step\n",
      "Epoch 17/30\n",
      "122/122 - 0s - loss: 0.1867 - accuracy: 0.9346 - val_loss: 0.1969 - val_accuracy: 0.9222 - 174ms/epoch - 1ms/step\n",
      "Epoch 18/30\n",
      "122/122 - 0s - loss: 0.1880 - accuracy: 0.9372 - val_loss: 0.1918 - val_accuracy: 0.9252 - 167ms/epoch - 1ms/step\n",
      "Epoch 19/30\n",
      "122/122 - 0s - loss: 0.1828 - accuracy: 0.9387 - val_loss: 0.1909 - val_accuracy: 0.9258 - 166ms/epoch - 1ms/step\n",
      "Epoch 20/30\n",
      "122/122 - 0s - loss: 0.1817 - accuracy: 0.9372 - val_loss: 0.1929 - val_accuracy: 0.9252 - 165ms/epoch - 1ms/step\n",
      "Epoch 21/30\n",
      "122/122 - 0s - loss: 0.1777 - accuracy: 0.9405 - val_loss: 0.1893 - val_accuracy: 0.9270 - 167ms/epoch - 1ms/step\n",
      "Epoch 22/30\n",
      "122/122 - 0s - loss: 0.1781 - accuracy: 0.9408 - val_loss: 0.1858 - val_accuracy: 0.9258 - 169ms/epoch - 1ms/step\n",
      "Epoch 23/30\n",
      "122/122 - 0s - loss: 0.1733 - accuracy: 0.9436 - val_loss: 0.1908 - val_accuracy: 0.9270 - 156ms/epoch - 1ms/step\n",
      "Epoch 24/30\n",
      "122/122 - 0s - loss: 0.1742 - accuracy: 0.9441 - val_loss: 0.1816 - val_accuracy: 0.9288 - 161ms/epoch - 1ms/step\n",
      "Epoch 25/30\n",
      "122/122 - 0s - loss: 0.1749 - accuracy: 0.9405 - val_loss: 0.1939 - val_accuracy: 0.9282 - 169ms/epoch - 1ms/step\n",
      "Epoch 26/30\n",
      "122/122 - 0s - loss: 0.1716 - accuracy: 0.9428 - val_loss: 0.1884 - val_accuracy: 0.9270 - 169ms/epoch - 1ms/step\n",
      "Epoch 27/30\n",
      "122/122 - 0s - loss: 0.1700 - accuracy: 0.9449 - val_loss: 0.1842 - val_accuracy: 0.9276 - 171ms/epoch - 1ms/step\n"
     ]
    }
   ],
   "source": [
    "#Main point of execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file = 'SMSSpamCollection'\n",
    "    print(\"\\nPART1\")\n",
    "    print(\"\\nSPAM classification using Traditional Machine Learning Classifiers from SKLearn\\n\\n\")\n",
    "    dataset = Dataset()\n",
    "    dataset.read_data(file)\n",
    "    df = dataset.preprocess()\n",
    "    X_train, X_test, y_train, y_test = dataset.split()\n",
    "    \n",
    "    spam_class_ml = Spam_Classification_ML(X_train, X_test, y_train, y_test)\n",
    "    models = spam_class_ml.prepare_models()\n",
    "    spam_class_ml.evaluate(models)\n",
    "    \n",
    "    print(\"\\n\\nPART2\")\n",
    "    print(\"\\nSPAM classification using Use GLOVE Word Embedding Layers with Deep Learning\\n\\n\")\n",
    "    spam_class_dl = Spam_Classification_DL(df, X_train, X_test, y_train, y_test)\n",
    "    spam_class_dl.glove()\n",
    "    spam_class_dl.model()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
